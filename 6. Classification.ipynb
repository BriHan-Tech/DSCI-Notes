{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a5cd841-021a-4d90-93bb-d606e654c58e",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d63f76-725f-4f95-8768-45860c3d9c8c",
   "metadata": {},
   "source": [
    "## Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3eb978-9f84-46d2-aa12-d6979e9aae82",
   "metadata": {},
   "source": [
    "**Binary Classification** - A classification where only two classes are involved. <br>\n",
    "**Multiclass Classification** - A classification where more than two classes are involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a44f34-7ddc-4989-b404-b61e24953076",
   "metadata": {},
   "source": [
    "## Important Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8994bb0-8b51-456d-a157-6713cb1d42f8",
   "metadata": {},
   "source": [
    "- ```forcats```\n",
    "    - forcats package enables us to easily manipulate factors in R.\n",
    "    - factors are a special categorical type of variable in R that are often used for class label data.\n",
    "- ```tidymodels```\n",
    "    - K-nearest neighbour algorithm is implemented in the parsnip package (included in tidymodels)\n",
    "    - tidymodels package collection also provides workflow.\n",
    "- ```parsnip```\n",
    "    - part of the ```tidyverse``` metapackage (included in tidyverse)\n",
    "    - The K-nearest neighbor algorithm is implemented in here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932529e-9875-4392-bffb-0541acc67006",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83e6b54-b934-43c0-8de8-27cb1d839065",
   "metadata": {},
   "source": [
    "### Dealing with dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f11e59-c987-4273-ac00-876b70e4778f",
   "metadata": {},
   "source": [
    "- ```glimpse(dataframe)```\n",
    "    - previews the dataframe, making it easier to inspect the data.\n",
    "- ```factor(col_name, levels = c(..., ..., ...))``` \n",
    "    - is used to encode a vector as a factor; allows you to specify the values, and whether they are ordered or not.\n",
    "    - first argument is the column you want to convert.\n",
    "    - second argument are the values/categories/levles that are ordered.\n",
    "- ```as.factor()```\n",
    "    - simply coerces an existing vector to a factor, if possible.\n",
    "- ```as_factor(column)``` \n",
    "    - used with mutate, and turns a vector from type ___ into type factor.\n",
    "    - converts the column/variable into a statistical categorical variable.\n",
    "- ```add_row(df, col_name_1 = ..., col_name_2 = ..., ..., col_name_n = ...)```\n",
    "    - creates and adds a row/observation to the df\n",
    "    - specify the name and respective values of each column of the df in the argument.\n",
    "- ```levels(vector)```\n",
    "    - factors have \"levels\" which we can think of as categories\n",
    "    - returns the name of each category in a column\n",
    "    - requires a vector as an argument (might need to use pull()\n",
    "- ```pull(dataframe, column)```\n",
    "    - pull allows us to extract a specific column.\n",
    "- ```dist()```\n",
    "    - finds the euclidean distance between the specified observations of the dataframe.\n",
    "    - used with ```slice()``` to firest obtain the rows and then result is piped into ```dist()```\n",
    "    - if there are more than 2 rows, the result is a matrix showing the dsitance between each row; pipe into ```as.matrix()``` to get the matrix\n",
    "- ```bind_cols(col_object, df)```\n",
    "    - binds the column (vector) in argument 1 to a dataframe in argument 2.\n",
    "- ```rename(df, new_col_name = old_col_name)```\n",
    "    - renames the column name\n",
    "    \n",
    "    \n",
    "```r\n",
    "dist_two_rows <- df %>%\n",
    "                 slice(1, 2) %>%\n",
    "                 select(col1, col2, col3) %>%\n",
    "                 dist()\n",
    "```\n",
    "\n",
    "***NOTE: ```slice(1,5)``` slices row 1 and 5, ```slice(1:5)``` slices row 1 to 5***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f0381-671c-41ff-bb86-ae30b2c5189d",
   "metadata": {},
   "source": [
    "### Scatter Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72c3719-c6ba-469a-a7c0-b4b57796ad50",
   "metadata": {},
   "source": [
    "```scale_color_manual(labels = c(\"1\", \"2\"), values = c(\"orange2\", \"steelblue2\"))``` - Visualizes the relationship between the factor and the predictor variables.\n",
    "\n",
    "<img src=\"media/scatter_plot_scale_color_manual.png\" width=\"200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095737c3-5886-433b-91b0-ede27f5320a6",
   "metadata": {},
   "source": [
    "## Classification with *K*-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1787a49-5076-4839-bab7-be894ebce319",
   "metadata": {},
   "source": [
    "The *K*-nearest neighbors classifier generally finds the *K* \"nearest\" or \"most similar\" observations in our training set, and then uses their label to make a prediction for the new observation's label.\n",
    "\n",
    "<div style=\"display:flex; flex-direction:row;\">\n",
    "    <div>\n",
    "        <p>When K = 1:</p>\n",
    "        <img src=\"media/k = 1.png\" width=\"400px\">\n",
    "    </div>\n",
    "    <div style=\"margin-left: 50px\">\n",
    "        <p>When K = 3:</p>\n",
    "        <img src=\"media/k = 3.png\" width=\"400px\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5345fb6d-9b34-48bd-8318-d57976c964d8",
   "metadata": {},
   "source": [
    "### Distance between points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fcaac7d-c86d-4fa9-a7ce-52e2733104a9",
   "metadata": {},
   "source": [
    "The distance from one point to another can be calculated by: $\\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2 + ... + (a_m - b_m)^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155a7b4-b992-44f5-a8c0-9aa179d9631e",
   "metadata": {},
   "source": [
    "To get the distances between our new observation and each of the observations in the training set to find *K = 5* neighbors:\n",
    "\n",
    "```R\n",
    "cancer |>\n",
    "  select(ID, Perimeter, Concavity, Symmetry, Class) |>\n",
    "  mutate(dist_from_new = sqrt((Perimeter - new_obs_Perimeter)^2 + \n",
    "                              (Concavity - new_obs_Concavity)^2 +\n",
    "                                (Symmetry - new_obs_Symmetry)^2)) |>\n",
    "  arrange(dist_from_new) |>\n",
    "  slice(1:5) # take the first 5 rows\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4f27dc-af02-4c34-beb4-6a59b941c200",
   "metadata": {},
   "source": [
    "## *K*-neighbors Classification Using ```tidymodels```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e03a1d2-5146-425b-8606-85f133be08bd",
   "metadata": {},
   "source": [
    "```library(tidymodels)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41644794-5c21-4372-b1e9-3ba77fec0ae1",
   "metadata": {},
   "source": [
    "### Creating a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a983ecc-5281-4cea-861a-b8254352e885",
   "metadata": {},
   "source": [
    "We create a ***model specification*** for *K*-nearest neighbors:\n",
    "\n",
    "```r\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 5) %>%\n",
    "            set_engine(\"kknn\") %>%\n",
    "            set_mode(\"classification\")\n",
    "```\n",
    "\n",
    "- ```weight_func = \"rectangular\"``` - specifies that we want the straight-line distance.\n",
    "- ```neighbors = 5``` - specifies we want *K* = 5\n",
    "- ```set_engine(\"kknn\")``` - specifies that we want to use the ```kknn``` package to train the model.\n",
    "- ```set_mode(\"classification\")``` - specifies that this is a classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc161046-36fe-4891-8491-f1b2a579c770",
   "metadata": {},
   "source": [
    "### Fitting the data to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35200351-1de5-42ce-beeb-3a63213222e0",
   "metadata": {},
   "source": [
    "In order to fit the model to the data, we need to pass the model specification and the data set to the ```fit``` function.\n",
    "\n",
    "```R\n",
    "knn_fit <- knn_spec %>%\n",
    "           fit(label ~ predictor + predictor, data = dataframe)\n",
    "```\n",
    "\n",
    "- ```label ~ predictor + predictor``` - specifies the label and the predictors.\n",
    "\n",
    "NOTE: You can use ```label ~ .``` to indicate that we want to use every variable **except** ```label``` which is the predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50bd3c0-3f01-495f-be1b-e9b1ba621e2e",
   "metadata": {},
   "source": [
    "### Predicting a new observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7425333d-79fd-43a1-a52e-fd3e5e0d7b8f",
   "metadata": {},
   "source": [
    "To predict a new obervation, we use the ```predict()``` function.\n",
    "\n",
    "```R\n",
    "new_obs <- tibble(Perimeter = 0, Concavity = 3.5)\n",
    "predict(knn_fit, new_obs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec30a0f-61d8-48dc-84eb-36f9fa84dfa6",
   "metadata": {},
   "source": [
    "## Data preprocessing with ```tidymodels```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733fd67a-e261-4374-9d42-16e5edb891e2",
   "metadata": {},
   "source": [
    "### Centering and Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9577a578-eb4d-4856-9afb-df5e7e94490e",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; flex-direction:row; width: 900px;\">\n",
    "    <p>\n",
    "        Imagine a dataset with salary and years of education. When computing the neighbor distances, a difference of $1000 is huge compared to a difference of 10 years of education; however, conceptually, we understand that its the opposite. 10 years of education is <b><i>huge</i></b> compared to a difference of one thousand dollars in yearly salary.\n",
    "To scale and center our data, we need to find our variable's mean and standard deviation. For each observed value of the variable, we subtract the mean and divide dby the standard deviation. When we do this, the data is said to be <i>standardized</i>, and all variables in a data set will have a mean of 0 and a standard deviation of 1.\n",
    "    </p>\n",
    "    <img src=\"media/scaled_vs_unscaled.png\" width=\"300px;\" style=\"margin-left: 100px;\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d726050-d877-4190-966c-7a0ee0446710",
   "metadata": {},
   "source": [
    "In ```tidymodels``` all data preprocessing happens using a ```recipe```.\n",
    "\n",
    "```R\n",
    "some_recipe <- recipe(label ~ ., data = unscaled_dataframe) %>%\n",
    "               step_scale(all_predictors()) %>%\n",
    "               step_center(all_predictors()) %>%\n",
    "               prep()\n",
    "```\n",
    "\n",
    "- ```step_scale``` and ```step_center``` - both centers and scales the data in a single recipe step.\n",
    "- ```prep``` - calculates the standard deviations and means required to scale and center the data. If you run the recipe before ```prep()```, it mentions the preprocessing steps it has to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a07c3-61e6-42a2-be66-2fbdd4e921db",
   "metadata": {},
   "source": [
    "Notably, there are other functions other than ```all_predictors()``` that can be used:\n",
    "- ```all_nomial()``` and ```all_numeric()``` - specifies all categorical or numeric valariables.\n",
    "- ```all_predictors()``` and ```all_outcomes()``` - specifies all predictor or target variables.\n",
    "- ```Area, Smoothnesss``` - just specifying the variables that should be scaled.\n",
    "- ```-Class``` - everything except for this variable.\n",
    "\n",
    "\n",
    "***NOTE: if some of our predictors are not numbers (qualitative), then we will need to use ```all_numeric_predictors()```***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730fca5-02d5-4fac-832c-f9cee84b80f2",
   "metadata": {},
   "source": [
    "To finish scaling the data, we use the ```bake``` function.\n",
    "\n",
    "```r\n",
    "scaled_dataframe <- bake(some_recipe, unscaled_dataframe)\n",
    "```\n",
    "\n",
    "```bake()``` - applies the result of ```prep()``` onto the unscaled dataframe and puts it to the scaled_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917d4ae9-7f69-4a0e-b702-42aefd5d10e2",
   "metadata": {},
   "source": [
    "### Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26531b26-c958-44d5-8654-c737c50cf5a3",
   "metadata": {},
   "source": [
    "***Class imbalance*** - where one label is much more common than the other.\n",
    "\n",
    "Class imbalance is a problem because *K*-nearest neighbor algorithm uses the labels of nearby points to predict the label of a new point; therefore, if there are many more data points with one label overall, the algorihtm is likely to pick that label in general.\n",
    "\n",
    "To fix this problem, we will ***oversample*** the rare class, replicating rare observations multiple times in our data set to give them more voign power in the *K*-nearest neighbor algorithm.\n",
    "\n",
    "```R\n",
    "library(themis)\n",
    "\n",
    "ups_recipe <- recipe(Class ~ ., data = rare_cancer) %>%\n",
    "              step_upsample(Class, over_ratio = 1, skip = FALSE) %>%\n",
    "              prep()\n",
    "\n",
    "upsampled_cancer <- bake(ups_recipe, rare_cancer)\n",
    "```\n",
    "\n",
    "<img src=\"media/class_imbalance.png\" width=\"700px;\">\n",
    "\n",
    "\n",
    "***NOTE:*** The ```prep()``` function makes the calculations while the ```bake()``` function adds the data to a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebdcc9a-5b71-4295-b334-381433ec7498",
   "metadata": {},
   "source": [
    "### Putting it together in a ```workflow```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3327731f-1ea3-46f3-a985-43cd5e6a0757",
   "metadata": {},
   "source": [
    "Workflows allow us to chain together multiple data analysis steps without intermediate steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d08d64f-66f5-4db5-9618-1b9c94114228",
   "metadata": {},
   "source": [
    "```R\n",
    "# load the unscaled cancer data \n",
    "# and make sure the target Class variable is a factor\n",
    "unscaled_cancer <- read_csv(\"data/unscaled_wdbc.csv\") |>\n",
    "  mutate(Class = as_factor(Class))\n",
    "\n",
    "# create the KNN model\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 7) |>\n",
    "  set_engine(\"kknn\") |>\n",
    "  set_mode(\"classification\")\n",
    "\n",
    "# create the centering / scaling recipe\n",
    "uc_recipe <- recipe(Class ~ Area + Smoothness, data = unscaled_cancer) |>\n",
    "  step_scale(all_predictors()) |>\n",
    "  step_center(all_predictors())\n",
    "\n",
    "# Using a workflow to chain together the data analysis steps\n",
    "knn_fit <- workflow() |>\n",
    "  add_recipe(uc_recipe) |>\n",
    "  add_model(knn_spec) |>\n",
    "  fit(data = unscaled_cancer)\n",
    "\n",
    "# predicting\n",
    "prediction <- predict(knn_fit, new_observation)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57fe250-b3bd-4b8d-94a2-3ec058eeb619",
   "metadata": {},
   "source": [
    "## Evaluating Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a944c57-9b6e-45aa-9a19-11442f2cfc17",
   "metadata": {},
   "source": [
    "To understand how well our classifier performs, we can start by splitting the data into a ***training set*** and ***testing set*** and only use the training set when building the classifier. Then, to evaluate the accuracy of the classifier, we set aside the true labels from the test set, and then use the classifier to predict the labels form the test set.\n",
    "\n",
    "$prediction \\space accuracy = \\frac{number \\space of \\space correct \\space predictions}{total \\space number \\space of \\space predictions}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e5fbd5-cce4-48e1-af94-e3213f03ff80",
   "metadata": {},
   "source": [
    "### Randomness and Seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac73877c-e3af-4076-8543-5919e8398a0a",
   "metadata": {},
   "source": [
    "#### Random Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669bd38-379c-4d90-9f10-cc95fce0e586",
   "metadata": {},
   "source": [
    "R's ```sample(num:num, number_in_list, replace = True)``` function can get us a list of random numbers.\n",
    "\n",
    "```r\n",
    "random_nums <- sample(0:9, 10, replace = TRUE)\n",
    "random_nums\n",
    "\n",
    "> [1] 4 9 5 9 6 8 4 4 8 8\n",
    "```\n",
    "\n",
    "Though the number looks random, it is determined by a seed value. We can set the seed value using ```set.seed(some_number)```. And by setting the seed value, we can make sample's output reproducible.\n",
    "\n",
    "```r\n",
    "set.seed(1)\n",
    "random_nums1 <- sample(0:9, 10, replace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b279ca-eb1b-4a5a-aedd-40faea8b3a3d",
   "metadata": {},
   "source": [
    "### Evaluating Accuracy with ```tidymodels```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86d5db0-e8ea-4dad-a1c1-907bd967db66",
   "metadata": {},
   "source": [
    "#### Splitting dataset to training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cac6905-cf60-4aa4-82a9-b556c1ede40a",
   "metadata": {},
   "source": [
    "The ```initial_split``` funtion handles the procedure of splitting the data. It ***shuffles*** the data beforre splitting (ensures the ordering present int he data does not influence the data that ends up in the training/testing sets), and ***stratifies*** the data by the class label (ensures that roughly the same proportion of each class is present in the training and testing set).\n",
    "\n",
    "```r\n",
    "dataset_split <- initial_split(data_set, prop = 0.75, strata = label)\n",
    "dataset_train <- training(dataset_split)\n",
    "dataset_test <- testing(dataset_split)\n",
    "```\n",
    "\n",
    "- ```prop``` specifies that proportion of the dataset that end up in the training set.\n",
    "- ```strata``` specifies the categorical variable (the label)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895ec7a-2ae4-4c27-9706-e2a25bc854c8",
   "metadata": {},
   "source": [
    "#### Dealing with the label in the testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd25ce59-6338-4867-82b9-0e20709a0465",
   "metadata": {},
   "source": [
    "After creating the predictions, we can use ```bind_cols(dataset)``` to merge the original testing set with the set of predictions.\n",
    "```r\n",
    "dataset_test_predictions <- predict(knn_fit dataset_test) %>%\n",
    "                            bind_cols(dataset_test)\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```r\n",
    "dataset_test_predictions <- dataset_test %>%\n",
    "                            bind_cols(predict(knn_fit dataset_test))\n",
    "```\n",
    "\n",
    "<img src=\"media/predicting_labels.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c18bc5-f4a2-4ce6-888c-191e30952734",
   "metadata": {},
   "source": [
    "#### Computing the Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe189b7-8a6f-4787-83c8-b85df0bef88b",
   "metadata": {},
   "source": [
    "Using the ```metrics```function, we can get the quality of our model.\n",
    "\n",
    "```r\n",
    "dataset_test_predictions %>%\n",
    "    metrics(truth = label, estimate = .pred_class) %>%\n",
    "    filter(.metric == \"accuracy\")\n",
    "```\n",
    "\n",
    "- ```truth``` - Argument specifies the label in the ```dataset_test_predictions``` that is true.\n",
    "- ```estimate``` - Argument specifies the predications that the model generated.\n",
    "\n",
    "<img src=\"media/classifier_metrics.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa1479-7f1e-4ae3-85fd-debb16c5e115",
   "metadata": {},
   "source": [
    "We an also look at the *confusion matrix* for the classifier, which shows the table of predicted labels and wrong labels, using ```conf_mat```:\n",
    "\n",
    "```r\n",
    "confusion <- dataset_predictions %>%\n",
    "             conf_mat(truth = label, estimate = .pred_class)\n",
    "```\n",
    "\n",
    "<img src=\"media/conf_mat.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd50fd9-5474-40c3-a303-4113ce3afae6",
   "metadata": {},
   "source": [
    "The confusion matrix is essentially a classification matrix, the columns of the confusion matrix represent the actual class and the rows represent the predicted class.\n",
    "\n",
    "<img src=\"media/confusion_matrix.png\" width=\"400px\">\n",
    "\n",
    "- A **true positive** is an outcome where the model correctly predicts the positive class.\n",
    "- A **true negative** is an outcome where the model correctly predicts the negative class.\n",
    "- A **false positive** is an outcome where the model incorrecly predicts the positive class.\n",
    "- A **false negative** is an outcome where the model incorrectly predicts the negative class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544391b8-243b-41c4-a357-4d305382c5f6",
   "metadata": {},
   "source": [
    "#### Critically Analyze Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28410a8f-dc33-4db2-8e5c-e6db08dfeb64",
   "metadata": {},
   "source": [
    "A *good* value for the accuracy depends on the application. For instance, if you are predicting whether a tumor is benign or malignant when it is benign 99% of the time, it is very easy to obtain a 99% accuracy just by guessing benign for each observation. It is also important to note the kind of mistakes the classifier is making. For instance, if it identifies the tumor as benign when it is malignant, this might mean the patient is not receiving enough medical care.\n",
    "\n",
    "You can compare your classifier to the ***majority classifier*** (which guesses the majority class label from the training data, regardless of the predictor variables' values), this helps gives you a sense when considering accuracies. If the majority classifier obtains a 90% accuracy on a problem, then you want your classifier to do better than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8503b49e-09d7-41b9-9fd7-3c3ba5d7564c",
   "metadata": {},
   "source": [
    "## Tuning the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601b4e0-249c-4933-a7a3-ea9706627a9b",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a639706-bd59-4482-a4fd-8290ed262953",
   "metadata": {},
   "source": [
    "- ```vfold_cv(training_dataframe, v = ..., strata = target_column)```\n",
    "    - This function splits our training data into $V$-folds automatically.\n",
    "    - This is to be done after data has been split onto ***training*** and ***test*** sets.\n",
    "    - Cross-validation uses a random process to select how to partition the training data. Use ```set.seed()``` to make it reproducable.\n",
    "- ```fit_resamples(..., resamples = df_vfold)```\n",
    "    - It is used instead of ```fit()``` when doiing cross-validation ***specifically for only specified neighbors***\n",
    "    - This runs cross-validation on each train/validation split\n",
    "    - first argument is the ```workflow()``` function which is piped in.\n",
    "- ```tune_grid(..., resamples = df_vfold, grid = n)```\n",
    "    - used instead of ```fit_resamples()``` function when doing cross-validation for $n$ neighbours.\n",
    "    - fits the model for each value in a range of parameter values\n",
    "    - third argument specifies that the tuning should try at most $n$ values of the number of neighbours $K$ when tuning.\n",
    "    - first argument is the ```workflow()``` which is piped in.\n",
    "    - We set the seed prior to tuning to ensure results are reproducible.\n",
    "- ```collect_metrics(...)```\n",
    "    - used instead of ```metrics()``` function when doing cross-validation.\n",
    "    - used to aggregate the mean and standard error of the classifier's validation accuracy across the folds.\n",
    "    - argument is the ```workflow()```\n",
    "- ```tune()```\n",
    "    - each parameter in the model to be tuned should be specified as ```tune()``` in the model specification rather than given a particular value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b20dcd4-7034-451d-b3c1-320e13c5bd42",
   "metadata": {},
   "source": [
    "In cross-validation, we split our overall training data into $C$ evenly sized chunks. Then, iteratively use 1 chunk as the validation set and combine the remaining $C-1$ chunks as the training set. Here, $C = 5$ different chunks of the data are used, resulting in 5 different choices for the validation set; this is called 5-fold cross-validation.\n",
    "\n",
    "$cross \\space validation \\space accuracy = \\frac{accuracy_1 + accuracy_2 + accuracy_3 + accuracy_4 + accuracy_5}{folds}$\n",
    "\n",
    "<img src=\"media/cross_validation.png\" width=\"400px\">\n",
    "\n",
    "To split our training data into v folds, we use ```vfold_cv```\n",
    "```r\n",
    "dataset_vfold <- vfold_cv(dataset_train, v = folds, strata = label)\n",
    "dataset_vfold\n",
    "```\n",
    "\n",
    "The ```strata``` argument makes sure that there is an even split for the label. (e.g. the label is ```am```, there would be a similar number of ```am```s and not ```am```s).\n",
    "\n",
    "<img src=\"media/vfold.png\" width=\"400px\">\n",
    "\n",
    "Then, to run the cross-validation on each train/validation split, we use ```fit_resamples``` function instead of ```fit``` in the workflow.\n",
    "\n",
    "```r\n",
    "dataset_recipe <- recipe(label ~ predictors, data = dataset) %>%\n",
    "                  step_scale(all_predictors()) %>%\n",
    "                  step_center(all_predictors())\n",
    "\n",
    "knn_fit <- workflow() %>%\n",
    "           add_recipe(dataset_recipe) %>%\n",
    "           add_model(knn_spec) %>%\n",
    "           fit_resamples(resamples = dataset_vfold)\n",
    "```\n",
    "\n",
    "We will then use ```collect_metrics``` to aggregate the mean and standard error.\n",
    "\n",
    "```r\n",
    "knn_fit %>% \n",
    "    collect_metrics()\n",
    "```\n",
    "\n",
    "<img src=\"media/metrics_of_cross_validation.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a660b5f4-e4d0-4ce2-af9a-719ce2c27bbe",
   "metadata": {},
   "source": [
    "We can choose any number of folds, and typically the more we folds, the better our accuracy estimate will be (lower standard error); however, we are limited by computational power, and hence the more time it takes to run the analysis. So, we usually choose 5 or 10 for $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea2aab-9b46-4861-8b3b-83d132327121",
   "metadata": {},
   "source": [
    "### Using the classifier to choose the value $K$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af4d60c-84ab-4d54-bf57-d3a9b7c2db06",
   "metadata": {},
   "source": [
    "We can use ```tune()``` in the parameters of the model to tune the model.\n",
    "\n",
    "```r\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) |>\n",
    "            set_engine(\"kknn\") |>\n",
    "            set_mode(\"classification\")\n",
    "```\n",
    "\n",
    "We create a list of the possible $K$'s to try using ```seq()``` and we pass the data frame to the ```grid``` argument of ```tune_grid```.\n",
    "\n",
    "```r\n",
    "k_vals <- tibble(neighbors = seq(from = 1, to = 100, by = 5))\n",
    "\n",
    "knn_results <- workflow() |>\n",
    "               add_recipe(dataframe_recipe) |>\n",
    "               add_model(knn_spec) |>\n",
    "               tune_grid(resamples = dataframe_vfold, grid = k_vals) |>\n",
    "               collect_metrics() \n",
    "\n",
    "accuracies <- knn_results |>\n",
    "  filter(.metric == \"accuracy\")\n",
    "\n",
    "accuracies\n",
    "```\n",
    "\n",
    "```tune_grid()``` computes a set of performance metrics (accuracy or RMSE) for a predefined set of tuning parameters that correspond to a model or recipe across one or more resamples of the data.\n",
    "\n",
    "\n",
    "<img src=\"media/choosing_k_1.png\" width=\"500px\">\n",
    "\n",
    "\n",
    "Then, we can plot the accuracy against $K$, showing us the optimal number, $K$.\n",
    "\n",
    "<img src=\"media/choosing_k_2.png\" width=\"200px\">\n",
    "\n",
    "When selecting $K$, we are looking for a value where:\n",
    "- we get roughly optimal accuracy, so that our model will be accurate.\n",
    "- changing the value to a nearby one doesn't decrease accuracy too much, so that our choice is reliable in the presence of uncertainty.\n",
    "- if $K$ is too large, predicting becomes expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09673618-da3b-4e0e-8314-26345bdef77b",
   "metadata": {},
   "source": [
    "### Under/Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ef99d-3679-48c7-9207-d061fa86645f",
   "metadata": {},
   "source": [
    "When you keep increasing $K$, the accuracy of the classifier starts decreasing. \n",
    "\n",
    "<img src=\"media/k_too_high.png\" width=\"200px\">\n",
    "\n",
    "**Underfitting**: If the model *isn't influenced enough* by the training data, it is said to **underfit** the data. As we increase the number of neighbors, more and more of the training observations (and those that are farther) get a \"say\" in what class of a new observation is. This causes an \"averaging effect\" to take place.\n",
    "\n",
    "**Overfitting**: In contrast, when we decrease the number of neighbours, each individual data point has a stronger and stronger vote regarding nearby points. Since the data is noisy, this causes a more \"jagged\" boundary corresponding to a *less simple* model. In the extreme, setting $K = 1$, then the classifier is essentiallly matching each new observation to its closest neighbor in the training set.\n",
    "\n",
    "<img src=\"media/under_overfitting.png\" width=\"500px\">\n",
    "<img src=\"media/over_underfitting.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02045cdf-d832-406e-9491-5deedd25a003",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d93f508-3f54-45f6-86ca-ea04de0eee7c",
   "metadata": {},
   "source": [
    "<img src=\"media/tuning_summary.png\" width=\"500px\">\n",
    "\n",
    "The overall workflow is as follows:\n",
    "1. First read the data into R and apply ```as_factor()``` on the column/varibale you want to choose as your target variable.\n",
    "2. Use the ```initial_split``` function to split the data into a training and test set. Set the ```strata``` argument to the class label variable. Put the test set aside for now.\n",
    "3. Use the ```vfold_cv``` function to split up the ***training data*** for cross-validation.\n",
    "4. Create a ```recipe``` that specifies the class label and predictors, as well as preprocessing steps for all variables. Pass the training data as the data argument of the recipe.\n",
    "5. Create a ```nearest_neighbors``` model specification, with ```neighbors = tune()```.\n",
    "6. Add the ```recipe``` and model specification to a ```workflow()```, and use the ```tune_grid``` function on the train/validation splits to estimate the classifier accuracy for a range of $K$ values.\n",
    "> ```tune_grid()``` and ```fit_resamples()``` are used for ***training*** whereas ```fit()``` is used for ***testing***. <br>\n",
    "> ```fit_resamples()``` can only be used when you want to test the performance ***using only one specified neighbour***. <br>\n",
    "> ```tune_grid()``` is a much better alternative as you can test the performance of different neighbours by performing cross validation for each neighbour.\n",
    "7. Pick a value of $K$ that yields a high accuracy estimate that doesn’t change much if you change $K$ to a nearby value.\n",
    "8. Make a new model specification for the best parameter value (i.e., $K$), and retrain the classifier using the ```fit``` function.\n",
    "9. Evaluate the estimated accuracy of the classifier on the test set using the ```predict``` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d9338-0a20-4c2c-bff4-32e563781bfe",
   "metadata": {},
   "source": [
    "**Strengths:** $K$-nearest neighbors classification:\n",
    "1. Simple and intuitive\n",
    "2. Requries few assumptions\n",
    "3. Works for binary and multi-class classification problems.\n",
    "\n",
    "**Weaknesses:** $K$-nearest neighbors classification\n",
    "1. Becomes very slow as the training data gets larger.\n",
    "2. May not perform well with a large number of predictors.\n",
    "3. May not perform well when classes are imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601246fb-54ab-444e-aab9-edec2f9fc367",
   "metadata": {},
   "source": [
    "## Predictor Variable Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c0401c-16e1-468f-b547-79fb8658dd23",
   "metadata": {},
   "source": [
    "With more irrelevant predictors, the model accuracy estimate decreases.\n",
    "\n",
    "<img src=\"media/model_accuracy_and_irrelevant_predictors.png\" width=\"400px\">\n",
    "\n",
    "Note that it still outperforms the baseline majority classifier because the tuning procedure for the $K$-nearest neighbors classifier combats the extra randomness from the irrelevant variables by increasing the number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b385899-1d86-43f1-9348-0e56067c3073",
   "metadata": {},
   "source": [
    "#### Finding a good subset of predictors (Method 1 - Best subset selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd1b926-aa2c-47db-9627-d07279b866d1",
   "metadata": {},
   "source": [
    "1. Create a separate model for every possible subset of predictors.\n",
    "2. Tune each one using cross-validation.\n",
    "3. Pick the subset of predictors that gives you the highest cross-validation accuracy.\n",
    "\n",
    "This is really expensive when it comes to computing power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d19609-750e-4ffa-a2ca-99885867f3aa",
   "metadata": {},
   "source": [
    "#### Finding a good subset of predictors (Method 2 - Forward Selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1b6194-8def-499b-82b2-d39fe14e381a",
   "metadata": {},
   "source": [
    "1. Start with a model with no predictors\n",
    "2. Run the following 3 steps until you run out of predictors.\n",
    "    1. For each unused predictor, add it to the model to form a *candidate* model.\n",
    "    2. Tune all of the candidate models.\n",
    "    3. Update the model to be the candidate model with the highest cross-validation accuracy.\n",
    "3. Select the model that provides the best trade-off between accuracy and simplicity.\n",
    "\n",
    "Starting with $m$ total predictors, in the first iteration, you make $m$ candidate models, each with 1 predictor. In the second iteration, you make $m-1$ candidate models, each with two predictors. This continues. You will end up training $\\frac{1}{2} m (m + 1)$ separate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d07940-2b0d-4582-bd99-66ac1b31d7ad",
   "metadata": {},
   "source": [
    "##### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db5bf4-c56f-4305-87ff-e53e4beaffa9",
   "metadata": {},
   "source": [
    "First, we use the ```select``` funciton to extract the \"total\" set of predictors that we are willing to work with.\n",
    "\n",
    "```r\n",
    "cancer_subset <- cancer_irrelevant |> \n",
    "  select(Class, \n",
    "         Smoothness, \n",
    "         Concavity, \n",
    "         Perimeter, \n",
    "         Irrelevant1, \n",
    "         Irrelevant2, \n",
    "         Irrelevant3)\n",
    "\n",
    "names <- colnames(cancer_subset |> select(-Class))\n",
    "```\n",
    "\n",
    "The key idea is to use the ```paste``` function is to create a model formula for each susbet of predictors for which we want to build a model. The ```collapse``` argument tells ```paste``` what to put between the items in the list; to make a formula, we need to put a ```+``` symbol between each variable.\n",
    "\n",
    "```r\n",
    "example_formula <- paste(\"Class\", \"~\", paste(names, collapse=\"+\"))\n",
    "example_formula\n",
    "\n",
    "# > ## [1] \"Class ~ Smoothness+Concavity+Perimeter+Irrelevant1+Irrelevant2+Irrelevant3\"\n",
    "```\n",
    "\n",
    "Finally, we need to write code that performs the task of sequentially finding the best predictor to add to the model. For each set of predictors to try, we construct a model formula, paste it into a ```recipe```, build a ```workflow``` that tunes a $K$-NN classifier using 5-fold cross-valdiation, and finally records the estimated accuracy.\n",
    "\n",
    "```r\n",
    "# create an empty tibble to store the results\n",
    "accuracies <- tibble(size = integer(), \n",
    "                     model_string = character(), \n",
    "                     accuracy = numeric())\n",
    "\n",
    "# create a model specification\n",
    "knn_spec <- nearest_neighbor(weight_func = \"rectangular\", \n",
    "                             neighbors = tune()) |>\n",
    "     set_engine(\"kknn\") |>\n",
    "     set_mode(\"classification\")\n",
    "\n",
    "# create a 5-fold cross-validation object\n",
    "cancer_vfold <- vfold_cv(cancer_subset, v = 5, strata = Class)\n",
    "\n",
    "# store the total number of predictors\n",
    "n_total <- length(names)\n",
    "\n",
    "# stores selected predictors\n",
    "selected <- c()\n",
    "\n",
    "# for every size from 1 to the total number of predictors\n",
    "for (i in 1:n_total) {\n",
    "    # for every predictor still not added yet\n",
    "    accs <- list()\n",
    "    models <- list()\n",
    "    for (j in 1:length(names)) {\n",
    "        # create a model string for this combination of predictors\n",
    "        preds_new <- c(selected, names[[j]])\n",
    "        model_string <- paste(\"Class\", \"~\", paste(preds_new, collapse=\"+\"))\n",
    "\n",
    "        # create a recipe from the model string\n",
    "        cancer_recipe <- recipe(as.formula(model_string), \n",
    "                                data = cancer_subset) |>\n",
    "                          step_scale(all_predictors()) |>\n",
    "                          step_center(all_predictors())\n",
    "\n",
    "        # tune the KNN classifier with these predictors, \n",
    "        # and collect the accuracy for the best K\n",
    "        acc <- workflow() |>\n",
    "          add_recipe(cancer_recipe) |>\n",
    "          add_model(knn_spec) |>\n",
    "          tune_grid(resamples = cancer_vfold, grid = 10) |>\n",
    "          collect_metrics() |>\n",
    "          filter(.metric == \"accuracy\") |>\n",
    "          summarize(mx = max(mean))\n",
    "        acc <- acc$mx |> unlist()\n",
    "\n",
    "        # add this result to the dataframe\n",
    "        accs[[j]] <- acc\n",
    "        models[[j]] <- model_string\n",
    "    }\n",
    "    jstar <- which.max(unlist(accs))\n",
    "    accuracies <- accuracies |> \n",
    "      add_row(size = i, \n",
    "              model_string = models[[jstar]], \n",
    "              accuracy = accs[[jstar]])\n",
    "    selected <- c(selected, names[[jstar]])\n",
    "    names <- names[-jstar]\n",
    "}\n",
    "accuracies\n",
    "\n",
    "```\n",
    "\n",
    "<img src=\"media/forward_selection.png\" width=\"600px\">\n",
    "\n",
    "In order to find the right model from the sequence, we will balance high accuracy and model simplicity. We will do that by looking for the local maxima.\n",
    "\n",
    "<img src=\"media/num_predictors_graph.png\" width=\"400px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626d48d7-bd5a-4dad-a60c-ee314402b7d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
