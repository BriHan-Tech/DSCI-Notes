{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05ea9e7f-eb2a-4ce7-be7b-80a64dd23b0e",
   "metadata": {},
   "source": [
    "# Quiz 2 Specifics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0f1799-efb1-4643-9746-b30170229360",
   "metadata": {},
   "source": [
    "### Pros and Cons of Knn Regression vs Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a748090-490a-479d-8b01-b7d4828b35fc",
   "metadata": {},
   "source": [
    "***ADVANTAGE OF LINEAR REGRESSION over KNN-REGRESSION:***\n",
    "\n",
    "1. KNN regression does NOT predict well beyond the range of the predictors in the training data. Linear regression can be used to address this problem.\n",
    "2. In KNN regression, the method gets significantly slower as the training dataset grows. Linear regression can be used to address this problem.\n",
    "3. In linear regression, standardization does not affect the fit (it does affect the coefficients in the equation, though!)\n",
    "4. A straight line can be defined by two numbers, the vertical intercept and the slope. The intercept tells us what the prediction is when all of the predictors are equal to 0; and the slope tells us what unit increase in the target/response variable we predict given a unit increase in the predictor variable. KNN regression, as simple as it is to implement and understand, has no such interpretability from its wiggly line.\n",
    "\n",
    "***Disadvantages of Linear Regression when compared to KNN-regression:***\n",
    "\n",
    "1. When the relationship between the target and the predictor is not linear, but instead some other shape (e.g. curved or oscillating). In these cases the prediction model from a simple linear regression will underfit (have high bias), meaning that model/predicted values does not match the actual observed values very well. Such a model would probably have a quite high  when assessing model goodness of fit on the training data and a quite high  when assessing model prediction quality on a test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402c738-b37b-4728-b9c4-d268365b4447",
   "metadata": {},
   "source": [
    "### Classification vs Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b94c937-e6ec-40ca-99f9-8b477a227afe",
   "metadata": {},
   "source": [
    "Regression predicts a continuous quantity while classification predicts discrete properties. Another way to look at it would be that Regression is used for quantitative quantities while classification is used for qualitative quantities. Or, yet another way, Regression is used for numerical quantities, while Classification is used for Categorical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb74103-2b06-4963-9023-f431f19a4836",
   "metadata": {},
   "source": [
    "### Importance of some mistakes (False-Positives vs False-Negatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbb9637-2ee0-4f68-8415-e69d72e5a7fb",
   "metadata": {},
   "source": [
    "Imagine your model is predicting between malignant and benign tumors. It would be A LOT worst if you were to predict a malignant tumor as a benign tumor (a False Negative) compared to the other way around. This is because a malignant tumor that is not addressed can cause more damage than a benign tumor addressed as a malignant tumor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b984e8-ba85-4e47-96da-c865a7a5b728",
   "metadata": {},
   "source": [
    "### Why is scaling important?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c384c4-3872-4bc1-ba14-729d092a041e",
   "metadata": {},
   "source": [
    "Imagine a dataset with salary and years of education. In the computation of the distances, a difference of $1000 is huge compared to a difference of 10 years of education; however, it is quite the opposite. 10 years of education is huge compared to a difference of one thousand dollars in yearly salary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9f7fa9-98a7-493e-ad76-360f71c10b26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importance in splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b4047-493c-4fc4-8956-f17743eae45f",
   "metadata": {},
   "source": [
    "It is important because if you put your testing dataset into your training dataset, you will not be able to accurately test how accurate your model can predict the data. If you put your training dataset and testing dataset together, your model's accuracy according to your (flawed) tests would be a lot higher than it actually is. (This is because your model is trained to recognize the data in the tests.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b646cbe-51c1-4054-abe7-8ae26bec92f3",
   "metadata": {},
   "source": [
    "### What is class imbalance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48027866-e315-4e67-9c1f-238cf7502c93",
   "metadata": {},
   "source": [
    "Class imbalance is a problem where there are more data points with one label overall, so the *K*-nearest neighbor algorithm is more likely to pick that label in general. To fix this problem, we will oversample the rare class, replicating the rare observations multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205f0df2-9067-43b9-bfa3-9d37e62a85bb",
   "metadata": {},
   "source": [
    "### What is Underfitting and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7656e4-3aea-45e7-8911-e578396b0033",
   "metadata": {},
   "source": [
    "Underfitting: The model isn't influenced enough by the training data, it is said to underfit the data. As we increase the number of neighbors, more and more of the training observations (and those that are farther) get a \"say\" in what class of a new observation is. This causes an \"averaging effect\" to take place.\n",
    "\n",
    "Overfitting: When we decrease the number of neighbours, each individual data point has a stronger and stronger say. Since the data is noisy, the model loses its simplicity. In the extreme, setting  ùêæ=1 , then the classifier is essentiallly matching each new observation to its closest neighbor in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc13a411-970b-4df0-8d41-25cd2b6449fe",
   "metadata": {},
   "source": [
    "### Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47512b2-bd9f-44e4-a301-f04560353af1",
   "metadata": {},
   "source": [
    "In k-nn regression with one outcome/target and one predictor variable, the predictions take form of a **wiggly/flexible plane**. <br>\n",
    "In multivariate k-nn regression with one outcome/target variable and two predictor variables, the prediction takes form of a **wiggly/flexible PLANE**. <br>\n",
    "In linear regression with one outcome/target and one predictor variable, the predictions take form of a **straight line**. <br>\n",
    "In multiple linear regression with one outcoke/target and two predictor variable, the predictions take form of a **flat plane**. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77177d4-f0f1-4459-a3ae-015009d2a982",
   "metadata": {},
   "source": [
    "### In your own words..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4ccb57-4532-4f5f-923a-c2f3653b86ba",
   "metadata": {},
   "source": [
    "#### Describe Knn Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07db1282-e06c-4467-9d69-7af54456d798",
   "metadata": {},
   "source": [
    "The K-nearest neighbours classification algorithm is an algorithm that predicts a categorical property for a point based on the K-nearest neighbours of the point (using predictors - assumably scaled). Using the neighbours, we can make a prediction about the point. For example, imagine you are trying to find whether a tumor is benign or malignant (a classification problem) using the area and depth of the tumor and five neighbors (assuming that five is the most optimal amount of neighbors). We will find the five closest points to the one you are trying to predict based on the scaled area and depth of the tumor. Based on whether the majority of the five points are malignant/benign, we can predict whether the point you are trying to predict is malignant or benign."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61126207-71dc-4c7f-b82f-32e6b9cc2160",
   "metadata": {},
   "source": [
    "#### Describe Knn Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effef36f-ba34-440b-8135-32d07e338a8a",
   "metadata": {},
   "source": [
    "Knn Regression is an algorithm where we use the K nearest neighbors to make a numerical prediction about a given point. For example, imagine we are trying to predict the price of houses based on their size. Let's say we have a graph of previous data points (on the x-axis is house sizes, and the y-axis is the price), and we want to predict how much a 2000-square-foot house would cost. Conceptually, the Knn regression algorithm draws a vertical line on the 2000 square foot part of the x-axis. Then, we average the K (number of neighbors) points closest to this line to find the estimated price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44402d45-8fc7-4db4-bf13-17dd9f7eed4b",
   "metadata": {},
   "source": [
    "#### Describe Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b38719-1b00-4bc0-a513-45adc3f97a00",
   "metadata": {},
   "source": [
    "Linear regression is an algorithm that draws a line of best fit to make a numerical prediction about a given point. For example, imagine we are trying to predict the price of houses based on their size. Let's say we have a graph of previous data points (on the x-axis is house sizes, and the y-axis is the price), and we want to predict how much a 2000-square-foot house would cost. Conceptually, the linear regression algorithm draws a line of best fit for all the previous data (this can be computed in a lot of ways, e.g., gradient descent), then using the line of best fit, we can find out the predicted y value of the x value 2000-square-foot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b083b-afdc-4ab9-970b-fa513e6e49f3",
   "metadata": {},
   "source": [
    "#### Describe Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e556724-fdcd-4897-b95f-5bd31ab78bbe",
   "metadata": {},
   "source": [
    "Cross-validation is a model validation technique that rotates out chunks (which are decided by the number of folds) of the training data, then iteratively uses the chunks to estimate how well the model will perform on the training data. In our case, we are using cross-validation to find the optimal number of neighbors. We use cross-validation because we can test what value K will perform best in our training set without running into the problem of double counting observations, testing on the training set (because we rotate out chunks of our training set to act as the testing set). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221b89f1-99e5-4d43-b3ca-9a165b55c7a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
